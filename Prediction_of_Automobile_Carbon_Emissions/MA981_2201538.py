# -*- coding: utf-8 -*-
"""MA981_2201538.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1niXUR-gQ_kujqwrdaI-yUYbglQm6DYED
"""

# Commented out IPython magic to ensure Python compatibility.
import sys
assert sys.version_info >= (3, 5)
import numpy as np
import os
import tarfile
import urllib
import pandas as pd
import urllib.request
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tabulate import tabulate

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

import os

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('./dissertation/') # setting the google drive path
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))

train = os.path.join(GOOGLE_DRIVE_PATH, 'CanadaCarEmissions.xlsx') # This is 100% of data

data = pd.read_excel(train)

data.head()

data.drop(0)

data.info()

data = data.dropna(subset=['CYLINDERS'])
data['CYLINDERS'] = data['CYLINDERS'].astype(int)

data = data.dropna(subset=['MODEL YEAR'])
data['MODEL YEAR'] = pd.to_datetime(data['MODEL YEAR'], format='%Y').dt.year

data = data.rename(columns={'MODEL(# = high output engine)': 'MODEL'})

data['MODEL'] = data['MODEL'].astype('category')

import pandas as pd
!pip install fancyimpute

from fancyimpute import IterativeImputer

import pandas as pd
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Assuming you have your data stored in a DataFrame called 'data'
data = pd.DataFrame(data)

# Columns for MICE imputation
columns_to_impute = ['CO2 Rating', 'Smog Rating']

# Perform MICE imputation
imputer = IterativeImputer(max_iter=100, random_state=0)
data[columns_to_impute] = imputer.fit_transform(data[columns_to_impute])

data.head()

# Assuming you have your data stored in a DataFrame called 'df'
data['CO2 Rating'] = data['CO2 Rating'].round(2)
data['Smog Rating'] = data['Smog Rating'].round(2)

print(data)

data["VEHICLE CLASS"].value_counts()

y_make = data.groupby('MAKE').size()/len(data)
data['MAKE_NEW'] = data['MAKE'].apply(lambda x_make : y_make[x_make])
data.drop(['MAKE'], axis=1, inplace=True)

data['MAKE_NEW'].head()

y_model = data.groupby('MODEL').size()/len(data)
data['MODEL_NEW'] = data['MODEL'].apply(lambda x_model : y_model[x_model])
data.drop(['MODEL'], axis=1, inplace=True)

data.head()

Categorical_df = data[['FUEL TYPE','VEHICLE CLASS','TRANSMISSION']]
Categorical_df.reset_index(inplace=True)
Categorical_df.drop(['index'], axis=1, inplace = True)
Categorical_df.head()

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first', sparse=False)

# Fit and transform the encoder
df_cat_encoded = encoder.fit_transform(Categorical_df)

# Get the column names for the encoded features
encoded_columns = encoder.get_feature_names_out(input_features=Categorical_df.columns)

# Create a new DataFrame with the encoded categorical features and the column names
df_cat = pd.DataFrame(df_cat_encoded, columns=encoded_columns)
df_cat.head()

print(df_cat.shape)

df = pd.concat([data,df_cat], axis=1)
df.head()

df.drop(['VEHICLE CLASS','TRANSMISSION','FUEL TYPE'], axis =1, inplace = True)
df.dropna(inplace=True)

df.drop(['MODEL YEAR', 'FUEL CONSUMPTION CITY (L/100)', 'FUEL CONSUMPTION HWY (L/100)', 'COMB (mpg)', 'Smog Rating'], axis=1, inplace=True)

df.head()

modeldata = df.copy()
modeldata.drop(['CO2 Rating'], axis=1, inplace=True)
modeldata.head()

temp = modeldata[['CO2 EMISSIONS (g/km)']]
modeldata.drop(['CO2 EMISSIONS (g/km)'], axis=1, inplace=True)
modeldata = pd.concat([modeldata, temp], axis=1)
modeldata.head()

modeldata.info()

X= modeldata.iloc[:,:69].values
y= modeldata.iloc[:,-1].values


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

from sklearn.linear_model import LinearRegression
regressor_mlr = LinearRegression()
regressor_mlr.fit(X_train, y_train)

from sklearn.model_selection import cross_val_score

# Calculate R-squared scores using cross-validation for training data
train_r2_scores = cross_val_score(regressor_mlr, X_train, y_train, cv=5, scoring='r2')
print("Training R-squared scores:", np.mean(train_r2_scores))

# Calculate R-squared scores using cross-validation for testing data
test_r2_scores = cross_val_score(regressor_mlr, X_test, y_test, cv=5, scoring='r2')
print("Testing R-squared scores:", np.mean(test_r2_scores))

from sklearn.ensemble import RandomForestRegressor
regressor_rf1 = RandomForestRegressor(n_estimators = 10, random_state = 0)
regressor_rf1.fit(X_train, y_train)


# Calculate R-squared scores using cross-validation for training data
train_r2_scores = cross_val_score(regressor_rf1, X_train, y_train, cv=5, scoring='r2')
print("Training R-squared scores:", np.mean(train_r2_scores))

# Calculate R-squared scores using cross-validation for testing data
test_r2_scores = cross_val_score(regressor_rf1, X_test, y_test, cv=5, scoring='r2')
print("Testing R-squared scores:", np.mean(test_r2_scores))

# XGB Regressor Model
import xgboost as xgb
xgb_model1 = XGBRegressor()
xgb_model1.fit(X_train, y_train)


# Calculate R-squared scores using cross-validation for training data
train_r2_scores = cross_val_score(xgb_model1, X_train, y_train, cv=5, scoring='r2')
print("Training R-squared scores:", np.mean(train_r2_scores))

# Calculate R-squared scores using cross-validation for testing data
test_r2_scores = cross_val_score(xgb_model1, X_test, y_test, cv=5, scoring='r2')
print("Testing R-squared scores:", np.mean(test_r2_scores))

# PCA
from sklearn. decomposition import PCA
pca = PCA (n_components=None)
X_train_pca1 = pca.fit_transform(X_train)
X_test_pca1 = pca.transform(X_test)
X_train_pca1.shape

import numpy as np
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

def find_optimal_pca_components(X_train, X_test, y_train, y_test, max_components=69):
    results = {'Num Components': [], 'Train R-squared': [], 'Test R-squared': []}

    for num_components in range(1, max_components + 1):
        pca = PCA(n_components=num_components)
        X_train_pca = pca.fit_transform(X_train)
        X_test_pca = pca.transform(X_test)

        regressor = LinearRegression()
        train_r2_scores = cross_val_score(regressor, X_train_pca, y_train, cv=5, scoring='r2')
        test_r2_scores = cross_val_score(regressor, X_test_pca, y_test, cv=5, scoring='r2')

        results['Num Components'].append(num_components)
        results['Train R-squared'].append(np.mean(train_r2_scores))
        results['Test R-squared'].append(np.mean(test_r2_scores))

    return results

# Example usage
optimal_pca_results = find_optimal_pca_components(X_train, X_test, y_train, y_test, max_components=69)

# Get the indices of the top 3 components with highest Test R-squared scores
top_3_train_indices = np.argsort(optimal_pca_results['Train R-squared'])[-3:]
# Get the indices of the top 3 components with highest Test R-squared scores
top_3_indices = np.argsort(optimal_pca_results['Test R-squared'])[-3:]

# Print the top 3 component indices and their scores
print("Top 3 Component Indices and Scores:")
for idx in top_3_indices:
    print(f"Component Index: {optimal_pca_results['Num Components'][idx]}, Test R-squared: {optimal_pca_results['Test R-squared'][idx]},,Train R-squared: {optimal_pca_results['Train R-squared'][idx]}")

pca.explained_variance_ratio_

np.cumsum(pca.explained_variance_ratio_)

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have already defined pca0 and its attributes

# Create a cumulative explained variance ratio plot
plt.plot(np.cumsum(pca.explained_variance_ratio_))

# Add a red horizontal line at y = 0.80
plt.axhline(y=0.80, color='red', linestyle='--')

# Find the index where cumulative explained variance ratio reaches 0.90
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)
index_90 = np.argmax(cumulative_variance >= 0.80)

# Add a red vertical line at the corresponding x-coordinate
plt.axvline(x=index_90, color='red', linestyle='--')

# Show the plot
plt.show()

# PCA
from sklearn. decomposition import PCA
pca = PCA (n_components=48)
X_train_pca7 = pca.fit_transform(X_train)
X_test_pca7 = pca.transform(X_test)
X_train_pca7.shape

from sklearn.linear_model import LinearRegression
regressor_mlr = LinearRegression()
regressor_mlr.fit(X_train_pca7, y_train)

from sklearn.model_selection import cross_val_score

# Calculate R-squared scores using cross-validation for training data
train_r2_scores = cross_val_score(regressor_mlr, X_train_pca7, y_train, cv=5, scoring='r2')
print("Training R-squared scores:", np.mean(train_r2_scores))

# Calculate R-squared scores using cross-validation for testing data
test_r2_scores = cross_val_score(regressor_mlr, X_test_pca7, y_test, cv=5, scoring='r2')
print("Testing R-squared scores:", np.mean(test_r2_scores))

regressor_mlr.coef_

# Make predictions on the training data
y_pred_test = regressor_mlr.predict(X_test_pca7)

# Calculate evaluation metrics
mae_test = mean_absolute_error(y_test, y_pred_test)
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(mse_test)


# Print the evaluation metrics
print("Mean Absolute Error (MAE) on training data:", mae_test)
print("Mean Squared Error (MSE) on training data:", mse_test)
print("Root Mean Squared Error (RMSE) on training data:", rmse_test)


# Calculate R-squared scores using cross-validation for training data
test_r2_scores = cross_val_score(regressor_mlr, X_test_pca7, y_test, cv=5, scoring='r2')
print("Cross-validated R-squared scores on training data:", np.mean(train_r2_scores))

y_pred = regressor_mlr.predict(X_test_pca7)

import matplotlib.pyplot as plt
import numpy as np


# Create a scatter plot of the actual vs. predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, c='blue', marker='o', label='Actual vs. Predicted')

# Plot a diagonal line to represent perfect predictions
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], c='red', linestyle='--', label='Perfect Predictions')

plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values (Linear Regression Model)')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.ensemble import RandomForestRegressor
regressor_rf7 = RandomForestRegressor(n_estimators = 10, random_state = 0)
regressor_rf7.fit(X_train_pca7, y_train)

# Calculate R-squared scores using cross-validation for training data
train_r2_scores = cross_val_score(regressor_rf7, X_train_pca7, y_train, cv=5, scoring='r2')
print("Training R-squared scores:", np.mean(train_r2_scores))

# Calculate R-squared scores using cross-validation for testing data
test_r2_scores = cross_val_score(regressor_rf7, X_test_pca7, y_test, cv=5, scoring='r2')
print("Testing R-squared scores:", np.mean(test_r2_scores))

# Make predictions on the training data
y_pred_test = regressor_rf7.predict(X_test_pca7)
# Calculate evaluation metrics
mae_test = mean_absolute_error(y_test, y_pred_test)
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(mse_test)



# Print the evaluation metrics
print("Mean Absolute Error (MAE) on testing data:", mae_test)
print("Mean Squared Error (MSE) on testing data:", mse_test)
print("Root Mean Squared Error (RMSE) on testing data:", rmse_test)

# XGB Regressor Model
import xgboost as xgb
xgb_model7 = XGBRegressor()
xgb_model7.fit(X_train_pca7, y_train)


# Calculate R-squared scores using cross-validation for training data
train_r2_scores = cross_val_score(xgb_model7, X_train_pca7, y_train, cv=5, scoring='r2')
print("Training R-squared scores:", np.mean(train_r2_scores))

# Calculate R-squared scores using cross-validation for testing data
test_r2_scores = cross_val_score(xgb_model7, X_test_pca7, y_test, cv=5, scoring='r2')
print("Testing R-squared scores:", np.mean(test_r2_scores))

# Make predictions on the training data
y_pred_train = xgb_model7.predict(X_train_pca7)

# Calculate evaluation metrics
mae_test = mean_absolute_error(y_test, y_pred_test)
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(mse_test)



# Print the evaluation metrics
print("Mean Absolute Error (MAE) on testing data:", mae_test)
print("Mean Squared Error (MSE) on testing data:", mse_test)
print("Root Mean Squared Error (RMSE) on testing data:", rmse_test)

import statsmodels.api as sm
def plot_qq_plot(model, X, y, title):
    y_pred = model.predict(X)
    residuals = y - y_pred
    sm.qqplot(residuals, line='s')
    plt.title(title)
    plt.show()



    # QQ Plot for each model
plot_qq_plot(regressor_mlr, X_test_pca7, y_test, 'QQ Plot for Linear Regression')
plot_qq_plot(regressor_rf7, X_test_pca7, y_test, 'QQ Plot for RandomForest Regressor')
plot_qq_plot(xgb_model7, X_test_pca7, y_test, 'QQ Plot for XGB Regressor')